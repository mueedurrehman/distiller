{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Resnet56Distiller.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0c384677c7b24cb3972ea38b951ae29d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_59cf3a3c93dc4a0ca84a5ae021b0394b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b385d6bbdbe5421a9698ed9b6282df77","IPY_MODEL_1b33ee27fc6646b9a67b3c1db57646b6"]}},"59cf3a3c93dc4a0ca84a5ae021b0394b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b385d6bbdbe5421a9698ed9b6282df77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0fe0ac2fca72438788d3b235d445cc64","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19b6ee81fd3b4b708735cd0ee411a2ca"}},"1b33ee27fc6646b9a67b3c1db57646b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e5b804b596714f329849fd1a593ea13d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170500096/? [00:20&lt;00:00, 100174178.85it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f343f84d83394cbaa3653f1a6469c294"}},"0fe0ac2fca72438788d3b235d445cc64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"19b6ee81fd3b4b708735cd0ee411a2ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5b804b596714f329849fd1a593ea13d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f343f84d83394cbaa3653f1a6469c294":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"x4aHvFV892tt"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import time\n","import math\n","from tqdm.autonotebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SB5m5Xaa5Omo","executionInfo":{"status":"ok","timestamp":1606089390491,"user_tz":300,"elapsed":30440,"user":{"displayName":"Mueed Ur Rehman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj9YH2JuHnytMBXOrP5AZ7UD35ohiAMKjIFnTh=s64","userId":"03998776733547559393"}},"outputId":"76253415-8e8f-43b1-d2c7-9244e5f9ddba"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["0c384677c7b24cb3972ea38b951ae29d","59cf3a3c93dc4a0ca84a5ae021b0394b","b385d6bbdbe5421a9698ed9b6282df77","1b33ee27fc6646b9a67b3c1db57646b6","0fe0ac2fca72438788d3b235d445cc64","19b6ee81fd3b4b708735cd0ee411a2ca","e5b804b596714f329849fd1a593ea13d","f343f84d83394cbaa3653f1a6469c294"]},"id":"lPa3hUZn6V3F","executionInfo":{"status":"ok","timestamp":1606089395551,"user_tz":300,"elapsed":35496,"user":{"displayName":"Mueed Ur Rehman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj9YH2JuHnytMBXOrP5AZ7UD35ohiAMKjIFnTh=s64","userId":"03998776733547559393"}},"outputId":"9e86c9f7-d17d-4d3d-b17b-accf61cfadcc"},"source":["transformTrain = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transformTest = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform = transformTrain)\n","\n","trainLoader = torch.utils.data.DataLoader(trainset, batch_size = 128,\n","                                          shuffle=True, num_workers=2)\n","\n","test = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform = transformTest)\n","testLoader = torch.utils.data.DataLoader(test, batch_size=128,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c384677c7b24cb3972ea38b951ae29d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"THO3463aJK44"},"source":["__all__ = ['resnet20_cifar', 'resnet32_cifar', 'resnet44_cifar', 'resnet56_cifar']\n","\n","NUM_CLASSES = 10\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n","                     padding=1, bias=False)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, block_gates, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.block_gates = block_gates\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.relu1 = nn.ReLU(inplace=False)  # To enable layer removal inplace must be False\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.relu2 = nn.ReLU(inplace=False)\n","        self.downsample = downsample\n","        self.stride = stride\n","        self.residual_eltwiseadd = EltwiseAdd()\n","\n","    def forward(self, x):\n","        residual = out = x\n","\n","        if self.block_gates[0]:\n","            out = self.conv1(x)\n","            out = self.bn1(out)\n","            out = self.relu1(out)\n","\n","        if self.block_gates[1]:\n","            out = self.conv2(out)\n","            out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out = self.residual_eltwiseadd(residual, out)\n","        out = self.relu2(out)\n","\n","        return out\n","\n","\n","class ResNetCifar(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=NUM_CLASSES):\n","        self.nlayers = 0\n","        # Each layer manages its own gates\n","        self.layer_gates = []\n","        for layer in range(3):\n","            # For each of the 3 layers, create block gates: each block has two layers\n","            self.layer_gates.append([])  # [True, True] * layers[layer])\n","            for blk in range(layers[layer]):\n","                self.layer_gates[layer].append([True, True])\n","\n","        self.inplanes = 16  # 64\n","        super(ResNetCifar, self).__init__()\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(self.layer_gates[0], block, 16, layers[0])\n","        self.layer2 = self._make_layer(self.layer_gates[1], block, 32, layers[1], stride=2)\n","        self.layer3 = self._make_layer(self.layer_gates[2], block, 64, layers[2], stride=2)\n","        self.avgpool = nn.AvgPool2d(8, stride=1)\n","        self.fc = nn.Linear(64 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, layer_gates, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(layer_gates[0], self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(layer_gates[i], self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","\n","def resnet20_cifar(**kwargs):\n","    model = ResNetCifar(BasicBlock, [3, 3, 3], **kwargs)\n","    return model\n","\n","def resnet32_cifar(**kwargs):\n","    model = ResNetCifar(BasicBlock, [5, 5, 5], **kwargs)\n","    return model\n","\n","def resnet44_cifar(**kwargs):\n","    model = ResNetCifar(BasicBlock, [7, 7, 7], **kwargs)\n","    return model\n","\n","def resnet56_cifar(**kwargs):\n","    model = ResNetCifar(BasicBlock, [9, 9, 9], **kwargs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DJw8FIOy7Dnq"},"source":["net = resnet56_cifar()\n","name = \"resnet-56-distiller\"\n","epochs = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEh7nsdeB5cv","executionInfo":{"status":"ok","timestamp":1606082213914,"user_tz":300,"elapsed":8733,"user":{"displayName":"Mueed Ur Rehman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj9YH2JuHnytMBXOrP5AZ7UD35ohiAMKjIFnTh=s64","userId":"03998776733547559393"}},"outputId":"4dbb6840-3310-4175-9abb-11db3eb3d3a4"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.backends.cudnn.benchmark = True\n","net = net.to(device)\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"hnZyYYFf9FtM","executionInfo":{"status":"error","timestamp":1606089258030,"user_tz":300,"elapsed":7052845,"user":{"displayName":"Mueed Ur Rehman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghj9YH2JuHnytMBXOrP5AZ7UD35ohiAMKjIFnTh=s64","userId":"03998776733547559393"}},"outputId":"5552542a-c37c-41ff-d84a-b8ad9c7e9b3f"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(net.parameters(), momentum = 0.9, weight_decay = 1e-4, nesterov=True, lr=0.3)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=45, gamma=0.1)\n","\n","# Training\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainLoader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","    return train_loss/len(trainLoader), 100.*correct/total\n","\n","best_acc = 0\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testLoader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        checkpoint = {\n","            'net': net.state_dict(),\n","            'arch' : 'resnet50',\n","            'optimizer_type' : torch.optim.SGD,\n","            'optimizer_state_dict' : optimizer.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch\n","        }\n","        torch.save(checkpoint, '/content/drive/MyDrive/Colab Notebooks/6787 Notebooks/models/' + name + '-' + str(epoch) + '-' + str(acc))\n","        best_acc = acc\n","    return acc\n","\n","\n","for epoch in range(epochs):\n","    startTime = time.time()\n","    trainLoss, trainAcc = train(epoch)\n","    testAcc = test(epoch)\n","    scheduler.step()\n","    endTime = time.time() - startTime\n","    print(\"Trn L: {:.4f}\".format(trainLoss) + \" \" + \"Trn A: {:.4f}\".format(trainAcc) + \" \" + \"Test A: {:.4f}\".format(testAcc) + str(endTime // 60) + \":\" + str(int(endTime % 60)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch: 0\n","Saving..\n","Trn L: 4.4565 Trn A: 10.4120 Test A: 13.00000.0:39\n","\n","Epoch: 1\n","Saving..\n","Trn L: 2.1199 Trn A: 18.1840 Test A: 21.49000.0:37\n","\n","Epoch: 2\n","Saving..\n","Trn L: 1.9348 Trn A: 24.7220 Test A: 26.70000.0:37\n","\n","Epoch: 3\n","Saving..\n","Trn L: 1.8077 Trn A: 30.0740 Test A: 34.23000.0:36\n","\n","Epoch: 4\n","Trn L: 1.7198 Trn A: 34.8440 Test A: 32.24000.0:36\n","\n","Epoch: 5\n","Saving..\n","Trn L: 1.6295 Trn A: 39.2820 Test A: 41.38000.0:37\n","\n","Epoch: 6\n","Saving..\n","Trn L: 1.5575 Trn A: 42.6480 Test A: 47.43000.0:36\n","\n","Epoch: 7\n","Saving..\n","Trn L: 1.4922 Trn A: 45.5680 Test A: 47.52000.0:37\n","\n","Epoch: 8\n","Saving..\n","Trn L: 1.4519 Trn A: 47.5040 Test A: 49.91000.0:37\n","\n","Epoch: 9\n","Saving..\n","Trn L: 1.4240 Trn A: 48.3720 Test A: 50.71000.0:37\n","\n","Epoch: 10\n","Saving..\n","Trn L: 1.3824 Trn A: 50.3360 Test A: 51.21000.0:37\n","\n","Epoch: 11\n","Trn L: 1.3104 Trn A: 53.2580 Test A: 38.86000.0:36\n","\n","Epoch: 12\n","Saving..\n","Trn L: 1.3102 Trn A: 53.1520 Test A: 54.13000.0:37\n","\n","Epoch: 13\n","Saving..\n","Trn L: 1.2204 Trn A: 56.7000 Test A: 55.63000.0:38\n","\n","Epoch: 14\n","Trn L: 1.1681 Trn A: 58.8460 Test A: 55.44000.0:36\n","\n","Epoch: 15\n","Saving..\n","Trn L: 1.1129 Trn A: 60.8680 Test A: 60.23000.0:37\n","\n","Epoch: 16\n","Trn L: 1.0707 Trn A: 62.5440 Test A: 59.35000.0:37\n","\n","Epoch: 17\n","Saving..\n","Trn L: 1.0357 Trn A: 63.9460 Test A: 65.78000.0:36\n","\n","Epoch: 18\n","Saving..\n","Trn L: 0.9985 Trn A: 65.1980 Test A: 66.89000.0:38\n","\n","Epoch: 19\n","Trn L: 0.9600 Trn A: 66.6180 Test A: 64.61000.0:37\n","\n","Epoch: 20\n","Saving..\n","Trn L: 0.9353 Trn A: 67.4880 Test A: 69.55000.0:37\n","\n","Epoch: 21\n","Trn L: 0.9065 Trn A: 68.8540 Test A: 66.61000.0:37\n","\n","Epoch: 22\n","Trn L: 0.8899 Trn A: 69.5660 Test A: 67.34000.0:36\n","\n","Epoch: 23\n","Trn L: 0.8709 Trn A: 70.3220 Test A: 69.15000.0:36\n","\n","Epoch: 24\n","Saving..\n","Trn L: 0.8462 Trn A: 70.8560 Test A: 70.79000.0:38\n","\n","Epoch: 25\n","Trn L: 0.8314 Trn A: 71.5280 Test A: 68.86000.0:37\n","\n","Epoch: 26\n","Trn L: 0.8192 Trn A: 71.8460 Test A: 67.39000.0:36\n","\n","Epoch: 27\n","Saving..\n","Trn L: 0.8041 Trn A: 72.6500 Test A: 70.87000.0:37\n","\n","Epoch: 28\n","Saving..\n","Trn L: 0.7925 Trn A: 73.2420 Test A: 72.37000.0:38\n","\n","Epoch: 29\n","Saving..\n","Trn L: 0.7897 Trn A: 73.1800 Test A: 74.38000.0:38\n","\n","Epoch: 30\n","Trn L: 0.7784 Trn A: 73.5620 Test A: 72.36000.0:36\n","\n","Epoch: 31\n","Trn L: 0.7669 Trn A: 73.6420 Test A: 69.58000.0:36\n","\n","Epoch: 32\n","Trn L: 0.7535 Trn A: 74.3280 Test A: 72.01000.0:36\n","\n","Epoch: 33\n","Trn L: 0.7495 Trn A: 74.6300 Test A: 73.78000.0:36\n","\n","Epoch: 34\n","Saving..\n","Trn L: 0.7337 Trn A: 75.0740 Test A: 75.49000.0:37\n","\n","Epoch: 35\n","Trn L: 0.7341 Trn A: 75.0840 Test A: 73.13000.0:37\n","\n","Epoch: 36\n","Trn L: 0.7232 Trn A: 75.2800 Test A: 72.91000.0:36\n","\n","Epoch: 37\n","Trn L: 0.7174 Trn A: 75.8120 Test A: 74.35000.0:37\n","\n","Epoch: 38\n","Trn L: 0.7070 Trn A: 76.0340 Test A: 72.46000.0:37\n","\n","Epoch: 39\n","Trn L: 0.7043 Trn A: 76.0100 Test A: 75.29000.0:36\n","\n","Epoch: 40\n","Trn L: 0.7000 Trn A: 76.3140 Test A: 74.33000.0:36\n","\n","Epoch: 41\n","Trn L: 0.6959 Trn A: 76.7260 Test A: 75.17000.0:36\n","\n","Epoch: 42\n","Trn L: 0.6883 Trn A: 76.8000 Test A: 75.04000.0:36\n","\n","Epoch: 43\n","Trn L: 0.6912 Trn A: 76.7080 Test A: 73.63000.0:36\n","\n","Epoch: 44\n","Saving..\n","Trn L: 0.6837 Trn A: 76.8540 Test A: 76.27000.0:37\n","\n","Epoch: 45\n","Saving..\n","Trn L: 0.5072 Trn A: 82.7920 Test A: 83.78000.0:39\n","\n","Epoch: 46\n","Saving..\n","Trn L: 0.4491 Trn A: 84.6220 Test A: 83.98000.0:38\n","\n","Epoch: 47\n","Saving..\n","Trn L: 0.4231 Trn A: 85.5540 Test A: 84.38000.0:39\n","\n","Epoch: 48\n","Saving..\n","Trn L: 0.4031 Trn A: 86.1020 Test A: 84.78000.0:38\n","\n","Epoch: 49\n","Saving..\n","Trn L: 0.3962 Trn A: 86.2700 Test A: 85.10000.0:38\n","\n","Epoch: 50\n","Saving..\n","Trn L: 0.3813 Trn A: 86.8260 Test A: 85.11000.0:39\n","\n","Epoch: 51\n","Trn L: 0.3734 Trn A: 87.1220 Test A: 84.94000.0:37\n","\n","Epoch: 52\n","Saving..\n","Trn L: 0.3659 Trn A: 87.4060 Test A: 85.42000.0:38\n","\n","Epoch: 53\n","Trn L: 0.3577 Trn A: 87.6080 Test A: 85.35000.0:37\n","\n","Epoch: 54\n","Trn L: 0.3524 Trn A: 87.8860 Test A: 85.30000.0:36\n","\n","Epoch: 55\n","Trn L: 0.3454 Trn A: 88.0120 Test A: 84.91000.0:37\n","\n","Epoch: 56\n","Trn L: 0.3362 Trn A: 88.3800 Test A: 85.34000.0:36\n","\n","Epoch: 57\n","Trn L: 0.3338 Trn A: 88.3580 Test A: 85.32000.0:36\n","\n","Epoch: 58\n","Trn L: 0.3248 Trn A: 88.6960 Test A: 85.20000.0:36\n","\n","Epoch: 59\n","Trn L: 0.3162 Trn A: 89.0620 Test A: 84.97000.0:37\n","\n","Epoch: 60\n","Trn L: 0.3147 Trn A: 88.9720 Test A: 85.20000.0:36\n","\n","Epoch: 61\n","Trn L: 0.3034 Trn A: 89.4160 Test A: 85.14000.0:36\n","\n","Epoch: 62\n","Saving..\n","Trn L: 0.3068 Trn A: 89.2440 Test A: 85.67000.0:37\n","\n","Epoch: 63\n","Trn L: 0.3058 Trn A: 89.3300 Test A: 85.54000.0:38\n","\n","Epoch: 64\n","Trn L: 0.2958 Trn A: 89.6740 Test A: 84.69000.0:36\n","\n","Epoch: 65\n","Trn L: 0.2947 Trn A: 89.8380 Test A: 85.52000.0:36\n","\n","Epoch: 66\n","Trn L: 0.2884 Trn A: 89.9660 Test A: 84.92000.0:36\n","\n","Epoch: 67\n","Trn L: 0.2856 Trn A: 90.1180 Test A: 85.09000.0:37\n","\n","Epoch: 68\n","Trn L: 0.2861 Trn A: 90.0240 Test A: 85.65000.0:36\n","\n","Epoch: 69\n","Trn L: 0.2806 Trn A: 90.2280 Test A: 84.99000.0:37\n","\n","Epoch: 70\n","Trn L: 0.2798 Trn A: 90.2580 Test A: 85.21000.0:37\n","\n","Epoch: 71\n","Trn L: 0.2764 Trn A: 90.4260 Test A: 85.17000.0:36\n","\n","Epoch: 72\n","Trn L: 0.2741 Trn A: 90.4260 Test A: 85.25000.0:36\n","\n","Epoch: 73\n","Trn L: 0.2757 Trn A: 90.3040 Test A: 85.12000.0:37\n","\n","Epoch: 74\n","Trn L: 0.2683 Trn A: 90.6140 Test A: 85.67000.0:36\n","\n","Epoch: 75\n","Trn L: 0.2651 Trn A: 90.7580 Test A: 85.08000.0:37\n","\n","Epoch: 76\n","Trn L: 0.2669 Trn A: 90.6620 Test A: 84.44000.0:36\n","\n","Epoch: 77\n","Trn L: 0.2614 Trn A: 90.7720 Test A: 85.07000.0:37\n","\n","Epoch: 78\n","Trn L: 0.2603 Trn A: 90.8220 Test A: 85.05000.0:36\n","\n","Epoch: 79\n","Saving..\n","Trn L: 0.2543 Trn A: 91.0660 Test A: 86.24000.0:37\n","\n","Epoch: 80\n","Trn L: 0.2564 Trn A: 91.0420 Test A: 84.93000.0:37\n","\n","Epoch: 81\n","Trn L: 0.2539 Trn A: 91.1220 Test A: 84.47000.0:36\n","\n","Epoch: 82\n","Trn L: 0.2536 Trn A: 91.1320 Test A: 84.92000.0:37\n","\n","Epoch: 83\n","Trn L: 0.2495 Trn A: 91.1940 Test A: 84.28000.0:36\n","\n","Epoch: 84\n","Trn L: 0.2513 Trn A: 91.2820 Test A: 84.61000.0:36\n","\n","Epoch: 85\n","Trn L: 0.2480 Trn A: 91.2260 Test A: 85.07000.0:36\n","\n","Epoch: 86\n","Trn L: 0.2447 Trn A: 91.4800 Test A: 84.90000.0:36\n","\n","Epoch: 87\n","Trn L: 0.2436 Trn A: 91.6340 Test A: 85.21000.0:37\n","\n","Epoch: 88\n","Trn L: 0.2501 Trn A: 91.0840 Test A: 85.00000.0:36\n","\n","Epoch: 89\n","Trn L: 0.2416 Trn A: 91.4760 Test A: 84.95000.0:36\n","\n","Epoch: 90\n","Saving..\n","Trn L: 0.1827 Trn A: 93.7020 Test A: 86.80000.0:37\n","\n","Epoch: 91\n","Trn L: 0.1584 Trn A: 94.5500 Test A: 86.50000.0:37\n","\n","Epoch: 92\n","Saving..\n","Trn L: 0.1459 Trn A: 94.9600 Test A: 86.83000.0:38\n","\n","Epoch: 93\n","Saving..\n","Trn L: 0.1382 Trn A: 95.2280 Test A: 87.14000.0:39\n","\n","Epoch: 94\n","Trn L: 0.1341 Trn A: 95.3840 Test A: 87.12000.0:37\n","\n","Epoch: 95\n","Trn L: 0.1305 Trn A: 95.4900 Test A: 86.96000.0:37\n","\n","Epoch: 96\n","Trn L: 0.1277 Trn A: 95.5320 Test A: 87.06000.0:36\n","\n","Epoch: 97\n","Saving..\n","Trn L: 0.1192 Trn A: 95.8180 Test A: 87.21000.0:36\n","\n","Epoch: 98\n","Trn L: 0.1216 Trn A: 95.7860 Test A: 87.08000.0:36\n","\n","Epoch: 99\n","Trn L: 0.1181 Trn A: 95.8700 Test A: 86.98000.0:35\n","\n","Epoch: 100\n","Trn L: 0.1150 Trn A: 95.9520 Test A: 87.11000.0:36\n","\n","Epoch: 101\n","Trn L: 0.1109 Trn A: 96.2140 Test A: 87.18000.0:36\n","\n","Epoch: 102\n","Trn L: 0.1094 Trn A: 96.1960 Test A: 87.19000.0:36\n","\n","Epoch: 103\n","Trn L: 0.1082 Trn A: 96.2160 Test A: 87.00000.0:37\n","\n","Epoch: 104\n","Trn L: 0.1043 Trn A: 96.4080 Test A: 87.21000.0:36\n","\n","Epoch: 105\n","Trn L: 0.0988 Trn A: 96.6460 Test A: 87.14000.0:35\n","\n","Epoch: 106\n","Trn L: 0.0979 Trn A: 96.6500 Test A: 87.08000.0:36\n","\n","Epoch: 107\n","Trn L: 0.0997 Trn A: 96.5000 Test A: 87.09000.0:37\n","\n","Epoch: 108\n","Saving..\n","Trn L: 0.0970 Trn A: 96.6340 Test A: 87.23000.0:37\n","\n","Epoch: 109\n","Saving..\n","Trn L: 0.0931 Trn A: 96.7620 Test A: 87.26000.0:37\n","\n","Epoch: 110\n","Saving..\n","Trn L: 0.0956 Trn A: 96.7200 Test A: 87.27000.0:38\n","\n","Epoch: 111\n","Trn L: 0.0905 Trn A: 96.8720 Test A: 87.02000.0:36\n","\n","Epoch: 112\n","Trn L: 0.0896 Trn A: 96.8840 Test A: 87.16000.0:36\n","\n","Epoch: 113\n","Trn L: 0.0884 Trn A: 96.8840 Test A: 87.27000.0:36\n","\n","Epoch: 114\n","Trn L: 0.0848 Trn A: 97.0220 Test A: 87.01000.0:36\n","\n","Epoch: 115\n","Trn L: 0.0847 Trn A: 97.0200 Test A: 87.21000.0:36\n","\n","Epoch: 116\n","Trn L: 0.0857 Trn A: 96.9900 Test A: 87.14000.0:36\n","\n","Epoch: 117\n","Trn L: 0.0850 Trn A: 97.0060 Test A: 87.09000.0:35\n","\n","Epoch: 118\n","Trn L: 0.0837 Trn A: 97.0600 Test A: 87.14000.0:37\n","\n","Epoch: 119\n","Trn L: 0.0809 Trn A: 97.2040 Test A: 87.15000.0:36\n","\n","Epoch: 120\n","Trn L: 0.0781 Trn A: 97.3160 Test A: 87.12000.0:37\n","\n","Epoch: 121\n","Trn L: 0.0805 Trn A: 97.1080 Test A: 87.04000.0:36\n","\n","Epoch: 122\n","Trn L: 0.0801 Trn A: 97.2320 Test A: 87.27000.0:37\n","\n","Epoch: 123\n","Trn L: 0.0749 Trn A: 97.3940 Test A: 87.12000.0:36\n","\n","Epoch: 124\n","Trn L: 0.0757 Trn A: 97.3940 Test A: 87.06000.0:36\n","\n","Epoch: 125\n","Trn L: 0.0748 Trn A: 97.4160 Test A: 87.23000.0:36\n","\n","Epoch: 126\n","Trn L: 0.0746 Trn A: 97.4000 Test A: 86.81000.0:36\n","\n","Epoch: 127\n","Trn L: 0.0737 Trn A: 97.4080 Test A: 86.96000.0:36\n","\n","Epoch: 128\n","Trn L: 0.0707 Trn A: 97.5760 Test A: 86.94000.0:36\n","\n","Epoch: 129\n","Trn L: 0.0687 Trn A: 97.5820 Test A: 86.85000.0:37\n","\n","Epoch: 130\n","Trn L: 0.0690 Trn A: 97.5780 Test A: 86.88000.0:37\n","\n","Epoch: 131\n","Trn L: 0.0696 Trn A: 97.5600 Test A: 87.10000.0:36\n","\n","Epoch: 132\n","Trn L: 0.0683 Trn A: 97.5980 Test A: 86.96000.0:36\n","\n","Epoch: 133\n","Trn L: 0.0668 Trn A: 97.7840 Test A: 87.21000.0:37\n","\n","Epoch: 134\n","Trn L: 0.0682 Trn A: 97.6600 Test A: 86.93000.0:36\n","\n","Epoch: 135\n","Trn L: 0.0622 Trn A: 97.8620 Test A: 86.99000.0:36\n","\n","Epoch: 136\n","Trn L: 0.0605 Trn A: 97.9680 Test A: 86.93000.0:36\n","\n","Epoch: 137\n","Trn L: 0.0587 Trn A: 97.9380 Test A: 87.10000.0:36\n","\n","Epoch: 138\n","Trn L: 0.0572 Trn A: 98.0860 Test A: 87.13000.0:36\n","\n","Epoch: 139\n","Trn L: 0.0558 Trn A: 98.1380 Test A: 87.10000.0:36\n","\n","Epoch: 140\n","Trn L: 0.0572 Trn A: 98.0380 Test A: 87.06000.0:36\n","\n","Epoch: 141\n","Trn L: 0.0569 Trn A: 98.0720 Test A: 87.19000.0:36\n","\n","Epoch: 142\n","Trn L: 0.0565 Trn A: 98.0620 Test A: 87.02000.0:36\n","\n","Epoch: 143\n","Trn L: 0.0544 Trn A: 98.1480 Test A: 87.27000.0:36\n","\n","Epoch: 144\n","Trn L: 0.0565 Trn A: 98.0360 Test A: 87.02000.0:36\n","\n","Epoch: 145\n","Trn L: 0.0563 Trn A: 98.0920 Test A: 87.04000.0:36\n","\n","Epoch: 146\n","Trn L: 0.0545 Trn A: 98.1740 Test A: 87.18000.0:36\n","\n","Epoch: 147\n","Trn L: 0.0518 Trn A: 98.2780 Test A: 87.09000.0:36\n","\n","Epoch: 148\n","Trn L: 0.0536 Trn A: 98.1780 Test A: 87.06000.0:36\n","\n","Epoch: 149\n","Trn L: 0.0533 Trn A: 98.1600 Test A: 87.08000.0:36\n","\n","Epoch: 150\n","Trn L: 0.0499 Trn A: 98.2700 Test A: 87.07000.0:36\n","\n","Epoch: 151\n","Trn L: 0.0517 Trn A: 98.2200 Test A: 87.12000.0:36\n","\n","Epoch: 152\n","Trn L: 0.0515 Trn A: 98.2440 Test A: 87.16000.0:36\n","\n","Epoch: 153\n","Trn L: 0.0511 Trn A: 98.2960 Test A: 87.18000.0:36\n","\n","Epoch: 154\n","Trn L: 0.0520 Trn A: 98.2040 Test A: 87.09000.0:36\n","\n","Epoch: 155\n","Trn L: 0.0531 Trn A: 98.2280 Test A: 87.15000.0:36\n","\n","Epoch: 156\n","Trn L: 0.0520 Trn A: 98.1600 Test A: 87.23000.0:36\n","\n","Epoch: 157\n","Trn L: 0.0518 Trn A: 98.2280 Test A: 87.25000.0:36\n","\n","Epoch: 158\n","Trn L: 0.0506 Trn A: 98.2960 Test A: 87.18000.0:36\n","\n","Epoch: 159\n","Saving..\n","Trn L: 0.0513 Trn A: 98.2500 Test A: 87.31000.0:37\n","\n","Epoch: 160\n","Trn L: 0.0502 Trn A: 98.3040 Test A: 87.21000.0:36\n","\n","Epoch: 161\n","Saving..\n","Trn L: 0.0517 Trn A: 98.2300 Test A: 87.33000.0:37\n","\n","Epoch: 162\n","Trn L: 0.0492 Trn A: 98.3680 Test A: 87.13000.0:37\n","\n","Epoch: 163\n","Trn L: 0.0520 Trn A: 98.2440 Test A: 87.16000.0:36\n","\n","Epoch: 164\n","Trn L: 0.0479 Trn A: 98.3420 Test A: 87.08000.0:36\n","\n","Epoch: 165\n","Trn L: 0.0510 Trn A: 98.2600 Test A: 87.11000.0:37\n","\n","Epoch: 166\n","Trn L: 0.0504 Trn A: 98.2820 Test A: 87.27000.0:37\n","\n","Epoch: 167\n","Trn L: 0.0490 Trn A: 98.3300 Test A: 87.14000.0:36\n","\n","Epoch: 168\n","Trn L: 0.0479 Trn A: 98.3700 Test A: 87.21000.0:36\n","\n","Epoch: 169\n","Trn L: 0.0486 Trn A: 98.4040 Test A: 87.17000.0:36\n","\n","Epoch: 170\n","Trn L: 0.0486 Trn A: 98.3300 Test A: 87.10000.0:36\n","\n","Epoch: 171\n","Trn L: 0.0469 Trn A: 98.4380 Test A: 87.20000.0:37\n","\n","Epoch: 172\n","Trn L: 0.0480 Trn A: 98.4060 Test A: 87.14000.0:37\n","\n","Epoch: 173\n","Trn L: 0.0486 Trn A: 98.3480 Test A: 87.13000.0:37\n","\n","Epoch: 174\n","Trn L: 0.0486 Trn A: 98.3520 Test A: 87.10000.0:36\n","\n","Epoch: 175\n","Trn L: 0.0487 Trn A: 98.3160 Test A: 87.20000.0:37\n","\n","Epoch: 176\n","Trn L: 0.0469 Trn A: 98.3840 Test A: 87.17000.0:36\n","\n","Epoch: 177\n","Trn L: 0.0475 Trn A: 98.3480 Test A: 87.14000.0:36\n","\n","Epoch: 178\n","Trn L: 0.0477 Trn A: 98.3280 Test A: 87.16000.0:36\n","\n","Epoch: 179\n","Trn L: 0.0472 Trn A: 98.3680 Test A: 87.11000.0:36\n","\n","Epoch: 180\n","Trn L: 0.0468 Trn A: 98.4440 Test A: 87.20000.0:36\n","\n","Epoch: 181\n","Trn L: 0.0479 Trn A: 98.3460 Test A: 87.12000.0:36\n","\n","Epoch: 182\n","Trn L: 0.0478 Trn A: 98.3520 Test A: 87.05000.0:36\n","\n","Epoch: 183\n","Trn L: 0.0494 Trn A: 98.3020 Test A: 87.12000.0:35\n","\n","Epoch: 184\n","Trn L: 0.0472 Trn A: 98.4000 Test A: 87.06000.0:35\n","\n","Epoch: 185\n","Trn L: 0.0470 Trn A: 98.3560 Test A: 87.10000.0:35\n","\n","Epoch: 186\n","Trn L: 0.0476 Trn A: 98.4080 Test A: 87.11000.0:36\n","\n","Epoch: 187\n","Trn L: 0.0487 Trn A: 98.3160 Test A: 87.16000.0:35\n","\n","Epoch: 188\n","Trn L: 0.0468 Trn A: 98.4200 Test A: 87.22000.0:37\n","\n","Epoch: 189\n","Trn L: 0.0455 Trn A: 98.4420 Test A: 87.12000.0:36\n","\n","Epoch: 190\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f41f383a4460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mtestAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-f41f383a4460>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-a4210facd29b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"wctLybM3846x"},"source":[""],"execution_count":null,"outputs":[]}]}